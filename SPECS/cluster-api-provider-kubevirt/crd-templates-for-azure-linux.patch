From 840a61881821823a4187a258f76f3d799c045c78 Mon Sep 17 00:00:00 2001
From: Sharath Srikanth Chellappa <sharathsr@microsoft.com>
Date: Sun, 10 Aug 2025 16:50:31 -0700
Subject: [PATCH] Adding new templates for AzureLinux

Adding KubevirtMachineTemplate, KubeadmControlPlane, and Cluster templates
for AzureLinux to support cluster creation with Azure Linux as the OS.

---
 templates/cluster-template-azl.yaml | 368 ++++++++++++++++++++++++++++
 1 file changed, 368 insertions(+)
 create mode 100644 templates/cluster-template-azl.yaml

diff --git a/templates/cluster-template-azl.yaml b/templates/cluster-template-azl.yaml
new file mode 100644
index 0000000..2d06618
--- /dev/null
+++ b/templates/cluster-template-azl.yaml
@@ -0,0 +1,368 @@
+---
+apiVersion: cluster.x-k8s.io/v1beta1
+kind: Cluster
+metadata:
+  name: "${CLUSTER_NAME}"
+  namespace: "${NAMESPACE}"
+spec:
+  clusterNetwork:
+    pods:
+      cidrBlocks:
+        - 10.243.0.0/16
+    services:
+      cidrBlocks:
+        - 10.95.0.0/16
+  infrastructureRef:
+    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+    kind: KubevirtCluster
+    name: '${CLUSTER_NAME}'
+    namespace: "${NAMESPACE}"
+  controlPlaneRef:
+    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
+    kind: KubeadmControlPlane
+    name: '${CLUSTER_NAME}-control-plane'
+    namespace: "${NAMESPACE}"
+---
+apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+kind: KubevirtCluster
+metadata:
+  name: "${CLUSTER_NAME}"
+  namespace: "${NAMESPACE}"
+spec:
+  controlPlaneServiceTemplate:
+    spec:
+      type: NodePort
+---
+apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+kind: KubevirtMachineTemplate
+metadata:
+  name: "${CLUSTER_NAME}-control-plane"
+  namespace: "${NAMESPACE}"
+spec:
+  template:
+    spec:
+      virtualMachineTemplate:
+        metadata:
+          namespace: "${NAMESPACE}"
+        spec:
+          dataVolumeTemplates:
+          - metadata:
+              name: root
+            spec:
+              pvc:
+                accessModes:
+                  - ReadWriteOnce
+                resources:
+                  requests:
+                    storage: ${OS_DISK_SIZE_GB}Gi
+                storageClassName: ${STORAGE_CLASS_NAME}
+                volumeMode: Filesystem
+              source:
+                pvc:
+                  name: ${OS_IMAGE_DATAVOLUME_PREFIX}
+                  namespace: ${CLUSTER_NAMESPACE}
+          running: true
+          template:
+            spec:
+              accessCredentials:
+                - userPassword:
+                    propagationMethod:
+                      qemuGuestAgent: {}
+                    source:
+                      secret:
+                        secretName: ${ACCESS_SECRET_PASSWORD:=""}
+                - sshPublicKey:
+                    propagationMethod:
+                      qemuGuestAgent:
+                        users:
+                          - clouduser
+                    source:
+                      secret:
+                        secretName: ${ACCESS_SECRET_PUBKEY:=""}
+              domain:
+                cpu:
+                  cores: ${KUBEVIRT_CONTROL_PLANE_CORES}
+                  model: host-passthrough
+                  sockets: 1
+                  threads: 1
+                devices:
+                  disks:
+                    - disk:
+                        bus: virtio
+                      name: root
+                    - name: cluster-sa-token
+                      disk:
+                        bus: sata
+                      serial: 6MNWHK43UMVZC243BFV2
+                  interfaces:
+                    - bridge: {}
+                      name: default
+                features:
+                  smm:
+                    enabled: ${OS_IMAGE_USE_SECURE_BOOT:=false}
+                firmware:
+                  bootloader:
+                    efi:
+                      secureBoot: ${OS_IMAGE_USE_SECURE_BOOT:=false}
+                resources:
+                  limits:
+                    cpu: ${KUBEVIRT_CONTROL_PLANE_CORES}
+                    memory: ${KUBEVIRT_CONTROL_PLANE_MEMORY}
+                  requests:
+                    cpu: ${KUBEVIRT_CONTROL_PLANE_CORES}
+                    memory: ${KUBEVIRT_CONTROL_PLANE_MEMORY}
+              evictionStrategy: External
+              volumes:
+                - dataVolume:
+                    name: root
+                  name: root
+                - name: cluster-sa-token
+                  secret:
+                    secretName: capkv-cluster-token
+              networks:
+                - name: default
+                  pod: {}
+---
+apiVersion: controlplane.cluster.x-k8s.io/v1beta1
+kind: KubeadmControlPlane
+metadata:
+  name: "${CLUSTER_NAME}-control-plane"
+  namespace: "${NAMESPACE}"
+spec:
+  kubeadmConfigSpec:
+    clusterConfiguration:
+      apiServer:
+        extraArgs:
+          cloud-provider: external
+        timeoutForControlPlane: 20m
+      controllerManager:
+        extraArgs:
+          bind-address: 0.0.0.0
+          cloud-provider: external
+          leader-elect-lease-duration: 60s
+          leader-elect-renew-deadline: 55s
+          terminated-pod-gc-threshold: "10"
+      etcd:
+        local:
+          imageRepository: mcr.microsoft.com/oss/etcd-io
+          imageTag: v3.5.6-13
+      imageRepository: ${CONTAINER_IMAGE_REGISTRY}
+      scheduler:
+        extraArgs:
+          bind-address: 0.0.0.0
+          leader-elect-lease-duration: 60s
+          leader-elect-renew-deadline: 55s
+    initConfiguration:
+      nodeRegistration:
+        criSocket: /var/run/containerd/containerd.sock
+        kubeletExtraArgs:
+          anonymous-auth: "false"
+          cgroup-driver: systemd
+          cloud-provider: external
+        name: '{{ ds.meta_data["local_hostname"] }}'
+    joinConfiguration:
+      nodeRegistration:
+        criSocket: /var/run/containerd/containerd.sock
+        kubeletExtraArgs:
+          cgroup-driver: systemd
+          cloud-provider: external
+        name: '{{ ds.meta_data["local_hostname"] }}'
+    postKubeadmCommands:
+      - mkdir /root/.kube
+      - cp /etc/kubernetes/admin.conf /root/.kube/config
+    preKubeadmCommands:
+      - export HOME=/root
+      - iptables -A INPUT -p tcp --dport 10250 -j ACCEPT -m comment --comment "Allow kubelet traffic"
+      - iptables -A INPUT -p tcp --dport 6443 -j ACCEPT -m comment --comment "Allow apiserver traffic"
+      - iptables -A INPUT -p tcp --dport 2379 -j ACCEPT -m comment --comment "Allow etcd traffic"
+      - iptables -A INPUT -p tcp --dport 2380 -j ACCEPT -m comment --comment "Allow etcd traffic"
+      - iptables -A INPUT -p tcp --dport 179 -j ACCEPT -m comment --comment "Allow bird traffic"
+      - iptables-save > /etc/systemd/scripts/ip4save
+      - echo kernel.pid_max=101382 > /etc/sysctl.d/55-pidmax.conf
+      - sysctl -p /etc/sysctl.d/55-pidmax.conf
+      - modprobe -- ip_vs
+      - modprobe -- ip_vs_rr
+      - modprobe -- ip_vs_wrr
+      - modprobe -- ip_vs_sh
+      - modprobe -- nf_conntrack
+      - |-
+        echo "---
+        apiVersion: kubeproxy.config.k8s.io/v1alpha1
+        kind: KubeProxyConfiguration
+        mode: ipvs" >> /run/kubeadm/kubeadm.yaml
+      - export SAMOUNT=/var/run/secrets/ecf/clusterserviceaccount
+      - export SADISK=/dev/disk/by-id/ata-QEMU_HARDDISK_6MNWHK43UMVZC243BFV2
+      - if [ -e $SADISK ]; then mkdir -p $SAMOUNT; echo $SADISK $SAMOUNT iso9660 ro,auto 0 0 >> /etc/fstab; mount $SAMOUNT; fi
+    useExperimentalRetryJoin: true
+    users:
+      - groups: sudo, docker
+        name: clouduser
+        shell: /bin/bash
+        sudo: ALL=(ALL) NOPASSWD:ALL
+  machineTemplate:
+    infrastructureRef:
+      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+      kind: KubevirtMachineTemplate
+      name: "${CLUSTER_NAME}-control-plane"
+      namespace: default
+  replicas: 1
+  version: v1.30.4
+---
+apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+kind: KubevirtMachineTemplate
+metadata:
+  name: "${CLUSTER_NAME}-md-0"
+  namespace: "${NAMESPACE}"
+spec:
+  template:
+    spec:
+      virtualMachineTemplate:
+        metadata:
+          namespace: "${NAMESPACE}"
+        spec:
+          dataVolumeTemplates:
+          - metadata:
+              name: root
+            spec:
+              pvc:
+                accessModes:
+                  - ReadWriteOnce
+                resources:
+                  requests:
+                    storage: ${OS_DISK_SIZE_GB}Gi
+                storageClassName: ${STORAGE_CLASS_NAME}
+                volumeMode: Filesystem
+              source:
+                pvc:
+                  name: ${OS_IMAGE_DATAVOLUME_PREFIX}
+                  namespace: ${CLUSTER_NAMESPACE}
+          running: true
+          template:
+            spec:
+              accessCredentials:
+                - userPassword:
+                    propagationMethod:
+                      qemuGuestAgent: {}
+                    source:
+                      secret:
+                        secretName: ${ACCESS_SECRET_PASSWORD:=""}
+                - sshPublicKey:
+                    propagationMethod:
+                      qemuGuestAgent:
+                        users:
+                          - clouduser
+                    source:
+                      secret:
+                        secretName: ${ACCESS_SECRET_PUBKEY:=""}
+              domain:
+                cpu:
+                  cores: ${KUBEVIRT_WORKER_CORES}
+                  model: host-passthrough
+                  sockets: 1
+                  threads: 1
+                devices:
+                  autoattachGraphicsDevice: false
+                  disks:
+                    - disk:
+                        bus: virtio
+                      name: root
+                    - name: cluster-sa-token
+                      disk:
+                        bus: sata
+                      serial: 6MNWHK43UMVZC243BFV2
+                  interfaces:
+                    - bridge: {}
+                      name: default
+                features:
+                  smm:
+                    enabled: false
+                firmware:
+                  bootloader:
+                    efi:
+                      secureBoot: false
+                resources:
+                  limits:
+                    cpu: ${KUBEVIRT_WORKER_CORES}
+                    memory: ${KUBEVIRT_WORKER_MEMORY}
+                  requests:
+                    cpu: ${KUBEVIRT_WORKER_CORES}
+                    memory: ${KUBEVIRT_WORKER_MEMORY}
+              evictionStrategy: External
+              volumes:
+                - dataVolume:
+                    name: root
+                  name: root
+                - name: cluster-sa-token
+                  secret:
+                    secretName: capkv-cluster-token
+              networks:
+                - name: default
+                  pod: {}
+---
+apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
+kind: KubeadmConfigTemplate
+metadata:
+  name: "${CLUSTER_NAME}-md-0"
+  namespace: "${NAMESPACE}"
+spec:
+  template:
+    spec:
+      clusterConfiguration:
+        etcd:
+          local:
+            imageRepository: mcr.microsoft.com/oss/etcd-io
+            imageTag: v3.5.6-13
+      joinConfiguration:
+        nodeRegistration:
+          kubeletExtraArgs:
+            cgroup-driver: systemd
+            cloud-provider: external
+            cpu-manager-policy: static
+            reserved-cpus: "0"
+          name: '{{ ds.meta_data["local_hostname"] }}'
+          criSocket: /var/run/containerd/containerd.sock
+      preKubeadmCommands:
+        - export HOME=/root
+        - iptables -A INPUT -p tcp --dport 10250 -j ACCEPT
+        - iptables -A INPUT -p tcp --dport 179 -j ACCEPT
+        - iptables-save > /etc/systemd/scripts/ip4save
+        - echo kernel.pid_max=359414 > /etc/sysctl.d/55-pidmax.conf
+        - sysctl -p /etc/sysctl.d/55-pidmax.conf
+        - modprobe -- ip_vs
+        - modprobe -- ip_vs_rr
+        - modprobe -- ip_vs_wrr
+        - modprobe -- ip_vs_sh
+        - modprobe -- nf_conntrack
+      useExperimentalRetryJoin: true
+      users:
+        - groups: sudo, docker
+          name: clouduser
+          shell: /bin/bash
+          sudo: ALL=(ALL) NOPASSWD:ALL
+---
+apiVersion: cluster.x-k8s.io/v1beta1
+kind: MachineDeployment
+metadata:
+  name: "${CLUSTER_NAME}-md-0"
+  namespace: "${NAMESPACE}"
+spec:
+  clusterName: "${CLUSTER_NAME}"
+  replicas: ${WORKER_MACHINE_COUNT}
+  selector:
+    matchLabels: null
+  template:
+    spec:
+      bootstrap:
+        configRef:
+          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
+          kind: KubeadmConfigTemplate
+          name: ${CLUSTER_NAME}-md-0
+          namespace: ${NAMESPACE}
+      clusterName: ${CLUSTER_NAME}
+      infrastructureRef:
+        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
+        kind: KubevirtMachineTemplate
+        name: ${CLUSTER_NAME}-md-0
+        namespace: ${NAMESPACE}
+      version: ${KUBERNETES_VERSION}
+---
\ No newline at end of file
-- 
2.49.0

